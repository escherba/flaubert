tokenizer:
    unicode_form: "NFKC"
    nltk_stop_words: "english"
    #sentence_tokenizer: ['nltk_data', 'tokenizers/punkt/english.pickle']
    sentence_tokenizer: ['data', 'sentence_tokenizer.pickle']
    max_char_repeats: 3
    lru_cache_size: 50000   # cache size (for wordnet lemmatizer only at the moment)
    html_renderer: "default"  # Either null or "default" or "beautifulsoup"
    translate_map_inv: {
        # normalize hashtags and mentions
        "@": "＠",
        '#': "＃",
        # replace fancy single apostrophes
        "'": "\u0060\u2018\u2019\u201a\u201b\u275b\u275c",
        # replace fancy double apostrophes
        '"': "\u201c\u201d\u201e\u201f\u275d\u275e",
        # multiplication sign
        'x': "\u00d7",
    }
    replace_map: {
        # n- and m-dash
        "\u2013": '--', "\u2014": '---'
    }
    add_abbrev_types: [
        'c.s.i', 'm.i.a', 'd.j', 'p.c', 'st', 'd.c', 'n.y.c', 'l.a', 's.f', 'bros',
        'v','co', 'u.s', 'dr', 'vs', 'jr', 'sr', 'ms', 'mr', 'mrs', 'prof', 'inc',
        'pron', 'col', 'lt', 'gen', 'sgt']
    del_sent_starters: [
        'seuss', 'strangelove', 'jekyll', 'moreau', 'hackenstein', 'blandings',
        'atoz', 'john']   # commonly prefixed with Dr. or St.

lemmatizer: "wordnet"   # null or "wordnet"
stemmer: null           # null or "porter"
embedding: "word2vec"    # either doc2vec or word2vec

word2vec:
    sg: 1               # 1 for skip-gram (default), cbow otherwise
    size: 400           # Number of features
    min_count: 40       # Minimum word count
    window: 20          # max distance between the current and predicted word
    sample: 0.001       # Downsample setting for frequent words
    seed: 1337          # for random number generator

doc2vec:
    dm: 1               # 1 for skip-gram (default), cbow otherwise
    size: 800           # Number of features
    min_count: 20       # Minimum word count
    window: 10          # max distance between the current and predicted word
    sample: 0.001       # Downsample setting for frequent words
    seed: 1337          # for random number generator
    train_words: true
    train_lbls: true

doc2vec_labels: ['sentence']   # allowed labels: ['sentence', 'document']
