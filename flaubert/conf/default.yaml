data:
    directory: 'data/twitter'
    labeled_fields: {'Y': 'Sentiment', 'X': 'SentimentText'}
    unlabeled_fields: {}
    train_labeled: 'twitter'
    train_unlabeled: ''
    test_final: ''
    extension: csv.gz
    read_csv_kwargs:
        header: 0
        delimiter: ","
        quotechar: '"'
        encoding: "utf-8"

tokenizer:
    features: ['CUSTOMTOKEN', 'REPLY', 'CENSORED', 'EMPHASIS_B', 'EMPHASIS_U', 'TIMEOFDAY', 'DATE', 'EMOTIC_EAST_LO', 'EMOTIC_EAST_HI', 'EMOTIC_EAST_SAD', 'EMOTIC_WEST_L', 'EMOTIC_WEST_R', 'EMOTIC_WEST_L_HAPPY', 'EMOTIC_WEST_CHEER', 'EMOTIC_WEST_L_MISC', 'EMOTIC_WEST_R_MISC', 'EMOTIC_RUSS_HAPPY', 'EMOTIC_RUSS_SAD', 'EMOTIC_HEART', 'CONTRACTION', 'MPAARATING', 'GRADE_POST', 'GRADE_PRE', 'THREED', 'NUMBER', 'HASHTAG', 'MENTION', 'ASCIIARROW_R', 'ASCIIARROW_L', 'MNDASH', 'ABBREV1', 'ABBREV2', 'ABBREV3', 'ELLIPSIS', 'XOXO', 'XX', 'PUNKT', 'ANYWORD']
    unicode_form: "NFKC"
    nltk_stop_words: "english"
    #sentence_tokenizer: ['nltk_data', 'tokenizers/punkt/english.pickle']
    sentence_tokenizer: ['data/aclImdb', 'sentence_tokenizer.pickle']
    max_char_repeats: 3
    lru_cache_size: 50000   # cache size (for wordnet lemmatizer only at the moment)
    unescape_html: true
    html_renderer: "default"  # Either null or "default" or "beautifulsoup"
    translate_map_inv: {
        # normalize hashtags and mentions
        "@": "＠",
        '#': "＃",
        # replace fancy single apostrophes
        "'": "\u0060\u2018\u2019\u201a\u201b\u275b\u275c",
        # replace fancy double apostrophes
        '"': "\u201c\u201d\u201e\u201f\u275d\u275e",
        # multiplication sign
        'x': "\u00d7",
    }
    replace_map: {
        # n- and m-dash
        "\u2013": '--', "\u2014": '---'
    }
    add_abbrev_types: [
        'c.s.i', 'm.i.a', 'd.j', 'p.c', 'st', 'd.c', 'n.y.c', 'l.a', 's.f', 'bros',
        'v','co', 'u.s', 'dr', 'vs', 'jr', 'sr', 'ms', 'mr', 'mrs', 'prof', 'inc',
        'pron', 'col', 'lt', 'gen', 'sgt']
    del_sent_starters: [
        'seuss', 'strangelove', 'jekyll', 'moreau', 'hackenstein', 'blandings',
        'atoz', 'john']   # commonly prefixed with Dr. or St.

preprocess:
    sample_labeled: null  # {0: 30000, 1: 30000}
    sample_unlabeled: null
    random_state: 0
    lemmatizer: "wordnet"   # null or "wordnet"
    stemmer: null           # null or "porter"

pretrain:
    split_by_sentence: false
    doc2vec_labels: ['sentence']   # allowed labels: ['sentence', 'document']
    algorithm: "glove"    # either word2vec or glove
    embedding: "word2vec"    # for word2vec algo, either word2vec or doc2vec

train:
    sample_labeled: {0: 30000, 1: 30000}
    test_size: 0.2
    random_state: 0
    split_by_sentence: false
    doc2vec_labels: []   # [] or ['document'] or ['sentence', 'document']
    classifier: 'svm'
    scoring: 'accuracy'
    features: ["bow", "embedding"]   # either "bow" or "embedding" or both
    nltk_stop_words: null    # either null or "english" or others...

glove:
    size: 400
    learning_rate: 0.04
    epochs: 20
    window: 40

word2vec:
    sg: 1               # 1 for skip-gram (default), cbow otherwise
    size: 400           # Number of features
    min_count: 40       # Minimum word count
    window: 20          # max distance between the current and predicted word
    sample: 0.001       # Downsample setting for frequent words
    seed: 1337          # for random number generator

doc2vec:
    dm: 1               # 1 for skip-gram (default), cbow otherwise
    size: 400           # Number of features
    min_count: 40       # Minimum word count
    window: 20          # max distance between the current and predicted word
    sample: 0.001       # Downsample setting for frequent words
    seed: 1337          # for random number generator
    train_words: true
    train_lbls: true
