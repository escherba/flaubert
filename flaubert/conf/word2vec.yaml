data:
    directory: 'data/aclImdb'
    labeled_fields: {'Y': 'sentiment', 'X': 'review'}
    unlabeled_fields: {'X': 'review'}
    train_labeled: 'labeledTrainData-*'
    train_unlabeled: 'unlabeledTrainData-*'
    test_final: 'testData'
    extension: tsv.gz
    read_csv_kwargs:
        header: 0
        quoting: 2
        delimiter: "\t"
        escapechar: "\\"
        quotechar: '"'
        encoding: "utf-8"

tokenizer:
    unicode_form: "NFKC"
    nltk_stop_words: "english"
    #sentence_tokenizer: ['nltk_data', 'tokenizers/punkt/english.pickle']
    sentence_tokenizer: ['data/aclImdb', 'sentence_tokenizer.pickle']
    max_char_repeats: 3
    lru_cache_size: 50000   # cache size (for wordnet lemmatizer only at the moment)
    html_renderer: "default"  # Either null or "default" or "beautifulsoup"
    translate_map_inv: {
        # normalize hashtags and mentions
        "@": "＠",
        '#': "＃",
        # replace fancy single apostrophes
        "'": "\u0060\u2018\u2019\u201a\u201b\u275b\u275c",
        # replace fancy double apostrophes
        '"': "\u201c\u201d\u201e\u201f\u275d\u275e",
        # multiplication sign
        'x': "\u00d7",
    }
    replace_map: {
        # n- and m-dash
        "\u2013": '--', "\u2014": '---'
    }
    add_abbrev_types: [
        'c.s.i', 'm.i.a', 'd.j', 'p.c', 'st', 'd.c', 'n.y.c', 'l.a', 's.f', 'bros',
        'v','co', 'u.s', 'dr', 'vs', 'jr', 'sr', 'ms', 'mr', 'mrs', 'prof', 'inc',
        'pron', 'col', 'lt', 'gen', 'sgt']
    del_sent_starters: [
        'seuss', 'strangelove', 'jekyll', 'moreau', 'hackenstein', 'blandings',
        'atoz', 'john']   # commonly prefixed with Dr. or St.

preprocess:
    lemmatizer: "wordnet"   # null or "wordnet"
    stemmer: null           # null or "porter"

pretrain:
    split_by_sentence: false
    doc2vec_labels: ['sentence']   # allowed labels: ['sentence', 'document']
    algorithm: "word2vec"    # either word2vec or glove
    embedding: "word2vec"    # for word2vec algo, either word2vec or doc2vec

train:
    test_size: 0.2
    random_state: 0
    split_by_sentence: false
    doc2vec_labels: []   # [] or ['document'] or ['sentence', 'document']
    classifier: 'svm'
    scoring: 'accuracy'
    features: ["embedding"]   # either "bow" or "embedding" or both
    nltk_stop_words: null    # either null or "english" or others...

glove:
    size: 400
    learning_rate: 0.04
    epochs: 20
    window: 40

word2vec:
    sg: 1               # 1 for skip-gram (default), cbow otherwise
    size: 400           # Number of features
    min_count: 40       # Minimum word count
    window: 20          # max distance between the current and predicted word
    sample: 0.001       # Downsample setting for frequent words
    seed: 1337          # for random number generator

doc2vec:
    dm: 1               # 1 for skip-gram (default), cbow otherwise
    size: 400           # Number of features
    min_count: 40       # Minimum word count
    window: 20          # max distance between the current and predicted word
    sample: 0.001       # Downsample setting for frequent words
    seed: 1337          # for random number generator
    train_words: true
    train_lbls: true
